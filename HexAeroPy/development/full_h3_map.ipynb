{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2046d51f-65da-4dac-bc84-665923e5369c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting spark.hadoop.yarn.resourcemanager.principal to quinten.goens\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<a href=\"https://spark-fepqbh1z5j0ehx06.cml-dev.az-dev.x9er-zkvz.cloudera.site\">https://spark-fepqbh1z5j0ehx06.cml-dev.az-dev.x9er-zkvz.cloudera.site</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hive Session ID = 23e3ba3f-64c1-4e2c-9dad-3c332d3f69ca\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from pyspark.sql.types import DoubleType, ArrayType, StructType, StructField, StringType, IntegerType, FloatType\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf,from_unixtime, min, max, to_date, pandas_udf, col, PandasUDFType, lit, round\n",
    "from pyspark.sql.types import DoubleType, ArrayType, StructType, StructField, StringType, IntegerType, FloatType\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import Window\n",
    "\n",
    "import os, time\n",
    "import subprocess\n",
    "import os,shutil\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "import requests\n",
    "from shapely.geometry import LineString, Polygon\n",
    "from shapely.ops import transform\n",
    "import pyproj\n",
    "from functools import partial\n",
    "from shapely.geometry import LineString\n",
    "from shapely.ops import transform\n",
    "from pyproj import Proj, Transformer\n",
    "import pandas as pd\n",
    "import folium\n",
    "from shapely.geometry import Polygon\n",
    "from shapely.ops import unary_union\n",
    "import shapely.geometry\n",
    "import h3\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Settings\n",
    "project = \"project_aiu\"\n",
    "\n",
    "\n",
    "# Getting today's date\n",
    "today = datetime.today().strftime('%d %B %Y')\n",
    "\n",
    "# Spark Session Initialization\n",
    "shutil.copy(\"/runtime-addons/cmladdon-2.0.40-b150/log4j.properties\", \"/etc/spark/conf/\") # Setting logging properties\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"OSN ADEP ADES Identification\") \\\n",
    "    .config(\"spark.log.level\", \"ERROR\")\\\n",
    "    .config(\"spark.hadoop.fs.azure.ext.cab.required.group\", \"eur-app-aiu-dev\") \\\n",
    "    .config(\"spark.kerberos.access.hadoopFileSystems\", \"abfs://storage-fs@cdpdldev0.dfs.core.windows.net/data/project/aiu.db/unmanaged\") \\\n",
    "    .config(\"spark.driver.cores\", \"1\") \\\n",
    "    .config(\"spark.driver.memory\", \"8G\") \\\n",
    "    .config(\"spark.executor.memory\", \"5G\") \\\n",
    "    .config(\"spark.executor.cores\", \"1\") \\\n",
    "    .config(\"spark.executor.instances\", \"2\") \\\n",
    "    .config(\"spark.dynamicAllocation.maxExecutors\", \"6\") \\\n",
    "    .config(\"spark.network.timeout\", \"800s\") \\\n",
    "    .config(\"spark.executor.heartbeatInterval\", \"400s\") \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Get environment variables\n",
    "engine_id = os.getenv('CDSW_ENGINE_ID')\n",
    "domain = os.getenv('CDSW_DOMAIN')\n",
    "\n",
    "# Format the URL\n",
    "url = f\"https://spark-{engine_id}.{domain}\"\n",
    "\n",
    "# Display the clickable URL\n",
    "display(HTML(f'<a href=\"{url}\">{url}</a>'))\n",
    "\n",
    "airports_df = spark.sql(f\"\"\"\n",
    "    SELECT id, ident, iso_country, continent, latitude_deg, longitude_deg, elevation_ft, type\n",
    "    FROM {project}.oa_airports\n",
    "    WHERE (type = 'large_airport' OR type = 'medium_airport');\n",
    "\"\"\")\n",
    "\n",
    "import math\n",
    "import json\n",
    "\n",
    "def generate_circle_polygon(lon, lat, radius_nautical_miles, num_points=360):\n",
    "    \"\"\"\n",
    "    Generate a polygon in GeoJSON format around a given latitude and longitude\n",
    "    with a specified radius in nautical miles.\n",
    "    \n",
    "    :param lat: Latitude of the center point\n",
    "    :param lon: Longitude of the center point\n",
    "    :param radius_nautical_miles: Radius in nautical miles\n",
    "    :param num_points: Number of points to generate for the polygon\n",
    "    :return: A dictionary representing the polygon in GeoJSON format\n",
    "    \"\"\"\n",
    "    # Convert radius from nautical miles to kilometers\n",
    "    radius_km = radius_nautical_miles * 1.852\n",
    "    \n",
    "    # Function to convert from degrees to radians\n",
    "    def degrees_to_radians(degrees):\n",
    "        return degrees * math.pi / 180\n",
    "    \n",
    "    # Function to calculate the next point given a distance and bearing\n",
    "    def calculate_point(lon, lat, distance_km, bearing):\n",
    "        R = 6371.01  # Earth's radius in kilometers\n",
    "        lat_rad = degrees_to_radians(lat)\n",
    "        lon_rad = degrees_to_radians(lon)\n",
    "        distance_rad = distance_km / R\n",
    "        bearing_rad = degrees_to_radians(bearing)\n",
    "        \n",
    "        lat_new_rad = math.asin(math.sin(lat_rad) * math.cos(distance_rad) +\n",
    "                                math.cos(lat_rad) * math.sin(distance_rad) * math.cos(bearing_rad))\n",
    "        lon_new_rad = lon_rad + math.atan2(math.sin(bearing_rad) * math.sin(distance_rad) * math.cos(lat_rad),\n",
    "                                           math.cos(distance_rad) - math.sin(lat_rad) * math.sin(lat_new_rad))\n",
    "                                           \n",
    "        lat_new = math.degrees(lat_new_rad)\n",
    "        lon_new = math.degrees(lon_new_rad)\n",
    "        return [lon_new, lat_new]\n",
    "    \n",
    "    # Generate points\n",
    "    points = []\n",
    "    for i in range(num_points):\n",
    "        bearing = 360 / num_points * i\n",
    "        point = calculate_point(lon, lat, radius_km, bearing)\n",
    "        points.append(point)\n",
    "    points.append(points[0])  # Close the polygon by repeating the first point\n",
    "    \n",
    "    # Create GeoJSON\n",
    "    geojson = {\n",
    "        \"type\": \"Polygon\",\n",
    "        \"coordinates\": [points]\n",
    "    }\n",
    "    \n",
    "    geojson_str = json.dumps(geojson)\n",
    "    \n",
    "    return geojson_str\n",
    "\n",
    "def fill_polygon_with_hexagons(polygon_json, resolution=8):\n",
    "    \"\"\"Fills a polygon defined by the given JSON with hexagons with defined resolution.\n",
    "\n",
    "    Args:\n",
    "        polygon_json (str): A JSON string defining the polygon.\n",
    "        resolution (int): The H3 resolution for the hexagons.\n",
    "\n",
    "    Returns:\n",
    "        list: The list contains the compact hexagon IDs.\n",
    "    \"\"\"\n",
    "    polygon = json.loads(polygon_json)\n",
    "    hexagons = h3.polyfill(polygon, resolution, geo_json_conformant=True)\n",
    "    return hexagons\n",
    "\n",
    "def fill_polygon_with_compact_hexagons(polygon_json, resolution=8):\n",
    "    \"\"\"Fills a polygon defined by the given JSON with compact hexagons.\n",
    "\n",
    "    Args:\n",
    "        polygon_json (str): A JSON string defining the polygon.\n",
    "        resolution (int): The H3 resolution for the hexagons.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing two lists. The first list contains the compact hexagon IDs,\n",
    "               and the second list contains the resolutions of these hexagons.\n",
    "    \"\"\"\n",
    "    hexagons = fill_polygon_with_hexagons(polygon_json, resolution)\n",
    "    compact_hexagons = list(h3.compact(hexagons))\n",
    "    compact_hexagons_resolutions = [h3.get_resolution(h) for h in compact_hexagons]\n",
    "    return compact_hexagons, compact_hexagons_resolutions\n",
    "\n",
    "generate_circle_polygon_udf = udf(generate_circle_polygon, StringType())\n",
    "\n",
    "compact_hex_result_type = StructType([\n",
    "    StructField(\"compact_hexagons\", ArrayType(StringType()), False),\n",
    "    StructField(\"compact_hexagons_resolutions\", ArrayType(IntegerType()), False)\n",
    "])\n",
    "\n",
    "fill_polygon_with_compact_hexagons_udf = udf(fill_polygon_with_compact_hexagons, compact_hex_result_type)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5ca32a6e-1d72-48c4-99c1-9ac11f0a4f11",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "num_points = 360\n",
    "max_resolution = 13\n",
    "radia_nm = [0, 5, 10, 15, 20, 30, 40, 50, 60, 70, 80, 90, 100]\n",
    "\n",
    "df = pd.DataFrame.from_dict({ #c stands for circle\n",
    "    'max_resolution' : max_resolution,\n",
    "    'number_of_points_c' : num_points, \n",
    "    'min_c_radius_nm' : radia_nm\n",
    "})\n",
    "df['max_c_radius_nm'] = df['min_c_radius_nm'].shift(-1).fillna(np.max(radia_nm)+10)\n",
    "\n",
    "df['min_c_radius_nm'] = df['min_c_radius_nm'].astype(float)\n",
    "df['max_c_radius_nm'] = df['max_c_radius_nm'].astype(float)\n",
    "df['m_col'] = 1\n",
    "\n",
    "# Define the schema corresponding to your Pandas DataFrame structure\n",
    "schema = StructType([\n",
    "    StructField(\"max_resolution\", IntegerType(), True),\n",
    "    StructField(\"number_of_points_c\", IntegerType(), True),\n",
    "    StructField(\"min_c_radius_nm\", FloatType(), True),\n",
    "    StructField(\"max_c_radius_nm\", FloatType(), True),\n",
    "    StructField(\"m_col\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "# Create PySpark DataFrame from Pandas DataFrame using the defined schema\n",
    "sdf = spark.createDataFrame(df, schema=schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aa0354d7-98ae-47ba-a690-8381577aeba7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "airports_df = airports_df.withColumn('m_col', lit(1)).join(sdf, on = 'm_col', how = 'left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "71a1a029-ce6c-4916-9028-9c9f280c9af0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/04/22 15:57:39 414 ERROR TaskSetManager: Task 1 in stage 2.0 failed 4 times; aborting job\n",
      "[Stage 2:=============================>                             (2 + 1) / 4]\r"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o252.collectToPython.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 2.0 failed 4 times, most recent failure: Lost task 1.3 in stage 2.0 (TID 11) (10.244.13.56 executor 3): java.net.SocketException: Connection reset\n\tat java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:115)\n\tat java.net.SocketOutputStream.write(SocketOutputStream.java:155)\n\tat java.io.BufferedOutputStream.write(BufferedOutputStream.java:122)\n\tat java.io.DataOutputStream.write(DataOutputStream.java:107)\n\tat java.io.FilterOutputStream.write(FilterOutputStream.java:97)\n\tat org.apache.spark.api.python.PythonRDD$.write$1(PythonRDD.scala:295)\n\tat org.apache.spark.api.python.PythonRDD$.$anonfun$writeIteratorToStream$1(PythonRDD.scala:307)\n\tat org.apache.spark.api.python.PythonRDD$.$anonfun$writeIteratorToStream$1$adapted(PythonRDD.scala:307)\n\tat scala.collection.Iterator.foreach(Iterator.scala:941)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1429)\n\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.writeIteratorToStream(PythonUDFRunner.scala:53)\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:435)\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2048)\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:270)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2454)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2403)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2402)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2402)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1160)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1160)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1160)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2642)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2584)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2573)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:938)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2214)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2235)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2254)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2279)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1030)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1029)\n\tat org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:410)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$executeCollect$1(AdaptiveSparkPlanExec.scala:343)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.withFinalPlanUpdate(AdaptiveSparkPlanExec.scala:371)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.executeCollect(AdaptiveSparkPlanExec.scala:343)\n\tat org.apache.spark.sql.Dataset.$anonfun$collectToPython$1(Dataset.scala:3538)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3706)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3704)\n\tat org.apache.spark.sql.Dataset.collectToPython(Dataset.scala:3535)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: java.net.SocketException: Connection reset\n\tat java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:115)\n\tat java.net.SocketOutputStream.write(SocketOutputStream.java:155)\n\tat java.io.BufferedOutputStream.write(BufferedOutputStream.java:122)\n\tat java.io.DataOutputStream.write(DataOutputStream.java:107)\n\tat java.io.FilterOutputStream.write(FilterOutputStream.java:97)\n\tat org.apache.spark.api.python.PythonRDD$.write$1(PythonRDD.scala:295)\n\tat org.apache.spark.api.python.PythonRDD$.$anonfun$writeIteratorToStream$1(PythonRDD.scala:307)\n\tat org.apache.spark.api.python.PythonRDD$.$anonfun$writeIteratorToStream$1$adapted(PythonRDD.scala:307)\n\tat scala.collection.Iterator.foreach(Iterator.scala:941)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1429)\n\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.writeIteratorToStream(PythonUDFRunner.scala:53)\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:435)\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2048)\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:270)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 31\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# Register the UDF\u001b[39;00m\n\u001b[1;32m     29\u001b[0m combine_polygons_udf \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mudf(combine_polygons, StringType())\n\u001b[0;32m---> 31\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43mairports_df\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwithColumn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minner_circle_polygon\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgenerate_circle_polygon_udf\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[43m        \u001b[49m\u001b[43mairports_df\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlongitude_deg\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[43m        \u001b[49m\u001b[43mairports_df\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlatitude_deg\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[43m        \u001b[49m\u001b[43mairports_df\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmin_c_radius_nm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[43m        \u001b[49m\u001b[43mairports_df\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumber_of_points_c\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwithColumn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mouter_circle_polygon\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgenerate_circle_polygon_udf\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[43m        \u001b[49m\u001b[43mairports_df\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlongitude_deg\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     42\u001b[0m \u001b[43m        \u001b[49m\u001b[43mairports_df\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlatitude_deg\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     43\u001b[0m \u001b[43m        \u001b[49m\u001b[43mairports_df\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_c_radius_nm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     44\u001b[0m \u001b[43m        \u001b[49m\u001b[43mairports_df\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumber_of_points_c\u001b[49m\n\u001b[1;32m     45\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwithColumn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcombined_circle_polygon\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     47\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcombine_polygons_udf\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     48\u001b[0m \u001b[43m        \u001b[49m\u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcol\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mouter_circle_polygon\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[43m        \u001b[49m\u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcol\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minner_circle_polygon\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     50\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwithColumn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     51\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhexagon_data\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfill_polygon_with_compact_hexagons_udf\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcombined_circle_polygon\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_resolution\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# Resolution\u001b[39;49;00m\n\u001b[1;32m     55\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwithColumn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     57\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhex_id\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     58\u001b[0m \u001b[43m        \u001b[49m\u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcol\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhexagon_data.compact_hexagons\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwithColumn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     60\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhex_resolution\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     61\u001b[0m \u001b[43m        \u001b[49m\u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcol\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhexagon_data.compact_hexagons_resolutions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     62\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtoPandas\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:157\u001b[0m, in \u001b[0;36mPandasConversionMixin.toPandas\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    154\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[1;32m    156\u001b[0m \u001b[38;5;66;03m# Below is toPandas without Arrow optimization.\u001b[39;00m\n\u001b[0;32m--> 157\u001b[0m pdf \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame\u001b[38;5;241m.\u001b[39mfrom_records(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m, columns\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns)\n\u001b[1;32m    158\u001b[0m column_counter \u001b[38;5;241m=\u001b[39m Counter(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns)\n\u001b[1;32m    160\u001b[0m dtype \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;01mNone\u001b[39;00m] \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mschema)\n",
      "File \u001b[0;32m/opt/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py:693\u001b[0m, in \u001b[0;36mDataFrame.collect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    683\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Returns all the records as a list of :class:`Row`.\u001b[39;00m\n\u001b[1;32m    684\u001b[0m \n\u001b[1;32m    685\u001b[0m \u001b[38;5;124;03m.. versionadded:: 1.3.0\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    690\u001b[0m \u001b[38;5;124;03m[Row(age=2, name='Alice'), Row(age=5, name='Bob')]\u001b[39;00m\n\u001b[1;32m    691\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    692\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m SCCallSiteSync(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sc) \u001b[38;5;28;01mas\u001b[39;00m css:\n\u001b[0;32m--> 693\u001b[0m     sock_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollectToPython\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    694\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(_load_from_socket(sock_info, BatchedSerializer(PickleSerializer())))\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/py4j/java_gateway.py:1304\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1298\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1299\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1300\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1301\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1303\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1304\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1305\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1307\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1308\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[0;32m/opt/spark/python/lib/pyspark.zip/pyspark/sql/utils.py:111\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw):\n\u001b[1;32m    110\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 111\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    112\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m py4j\u001b[38;5;241m.\u001b[39mprotocol\u001b[38;5;241m.\u001b[39mPy4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    113\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o252.collectToPython.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 2.0 failed 4 times, most recent failure: Lost task 1.3 in stage 2.0 (TID 11) (10.244.13.56 executor 3): java.net.SocketException: Connection reset\n\tat java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:115)\n\tat java.net.SocketOutputStream.write(SocketOutputStream.java:155)\n\tat java.io.BufferedOutputStream.write(BufferedOutputStream.java:122)\n\tat java.io.DataOutputStream.write(DataOutputStream.java:107)\n\tat java.io.FilterOutputStream.write(FilterOutputStream.java:97)\n\tat org.apache.spark.api.python.PythonRDD$.write$1(PythonRDD.scala:295)\n\tat org.apache.spark.api.python.PythonRDD$.$anonfun$writeIteratorToStream$1(PythonRDD.scala:307)\n\tat org.apache.spark.api.python.PythonRDD$.$anonfun$writeIteratorToStream$1$adapted(PythonRDD.scala:307)\n\tat scala.collection.Iterator.foreach(Iterator.scala:941)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1429)\n\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.writeIteratorToStream(PythonUDFRunner.scala:53)\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:435)\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2048)\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:270)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2454)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2403)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2402)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2402)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1160)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1160)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1160)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2642)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2584)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2573)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:938)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2214)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2235)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2254)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2279)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1030)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1029)\n\tat org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:410)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$executeCollect$1(AdaptiveSparkPlanExec.scala:343)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.withFinalPlanUpdate(AdaptiveSparkPlanExec.scala:371)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.executeCollect(AdaptiveSparkPlanExec.scala:343)\n\tat org.apache.spark.sql.Dataset.$anonfun$collectToPython$1(Dataset.scala:3538)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3706)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3704)\n\tat org.apache.spark.sql.Dataset.collectToPython(Dataset.scala:3535)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: java.net.SocketException: Connection reset\n\tat java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:115)\n\tat java.net.SocketOutputStream.write(SocketOutputStream.java:155)\n\tat java.io.BufferedOutputStream.write(BufferedOutputStream.java:122)\n\tat java.io.DataOutputStream.write(DataOutputStream.java:107)\n\tat java.io.FilterOutputStream.write(FilterOutputStream.java:97)\n\tat org.apache.spark.api.python.PythonRDD$.write$1(PythonRDD.scala:295)\n\tat org.apache.spark.api.python.PythonRDD$.$anonfun$writeIteratorToStream$1(PythonRDD.scala:307)\n\tat org.apache.spark.api.python.PythonRDD$.$anonfun$writeIteratorToStream$1$adapted(PythonRDD.scala:307)\n\tat scala.collection.Iterator.foreach(Iterator.scala:941)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1429)\n\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.writeIteratorToStream(PythonUDFRunner.scala:53)\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:435)\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2048)\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:270)\n"
     ]
    }
   ],
   "source": [
    "def combine_polygons(outer_geojson_str, inner_geojson_str):\n",
    "    \"\"\"\n",
    "    Combine two GeoJSON polygons into a single \"donut-shaped\" polygon.\n",
    "    \n",
    "    :param outer_geojson_str: GeoJSON string of the outer polygon\n",
    "    :param inner_geojson_str: GeoJSON string of the inner polygon\n",
    "    :return: GeoJSON string of the combined polygon with the inner area as a hole\n",
    "    \"\"\"\n",
    "    # Parse the GeoJSON strings into Python dictionaries\n",
    "    outer_geojson = json.loads(outer_geojson_str)\n",
    "    inner_geojson = json.loads(inner_geojson_str)\n",
    "    \n",
    "    # Extract the coordinates from the inner polygon\n",
    "    inner_coordinates = inner_geojson['coordinates']\n",
    "    \n",
    "    # Insert the inner coordinates as a hole in the outer polygon\n",
    "    combined_coordinates = outer_geojson['coordinates'] + [inner_coordinates[0]]\n",
    "    \n",
    "    # Create a new GeoJSON object for the combined polygon\n",
    "    combined_geojson = {\n",
    "        \"type\": \"Polygon\",\n",
    "        \"coordinates\": combined_coordinates\n",
    "    }\n",
    "    \n",
    "    # Convert the dictionary back to a JSON string\n",
    "    return json.dumps(combined_geojson)\n",
    "\n",
    "# Register the UDF\n",
    "combine_polygons_udf = F.udf(combine_polygons, StringType())\n",
    "\n",
    "res = airports_df.withColumn(\n",
    "    \"inner_circle_polygon\",\n",
    "    generate_circle_polygon_udf(\n",
    "        airports_df.longitude_deg,\n",
    "        airports_df.latitude_deg,\n",
    "        airports_df.min_c_radius_nm, \n",
    "        airports_df.number_of_points_c\n",
    "    )).withColumn(\n",
    "    \"outer_circle_polygon\",\n",
    "    generate_circle_polygon_udf(\n",
    "        airports_df.longitude_deg,\n",
    "        airports_df.latitude_deg,\n",
    "        airports_df.max_c_radius_nm, \n",
    "        airports_df.number_of_points_c\n",
    "    )).withColumn(\n",
    "    \"combined_circle_polygon\",\n",
    "    combine_polygons_udf(\n",
    "        F.col(\"outer_circle_polygon\"),\n",
    "        F.col(\"inner_circle_polygon\")\n",
    "    )).withColumn(\n",
    "        \"hexagon_data\",\n",
    "        fill_polygon_with_compact_hexagons_udf(\n",
    "            \"combined_circle_polygon\",\n",
    "            \"max_resolution\" # Resolution\n",
    "        )\n",
    "    ).withColumn(\n",
    "        \"hex_id\",\n",
    "        F.col(\"hexagon_data.compact_hexagons\")\n",
    "    ).withColumn(\n",
    "        \"hex_resolution\",\n",
    "        F.col(\"hexagon_data.compact_hexagons_resolutions\")\n",
    "    ).toPandas()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7216259-665a-433c-99e7-96267ff93640",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db50a1d0-da2f-4e0c-a716-c47fdcf1b740",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcaadcb3-2277-4de0-a072-2f2cf07508b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "pdfs = []\n",
    "failed_apts = []\n",
    "for radius_nm in radia_nm:\n",
    "    print(f\"Radius: {radius_nm}\")\n",
    "    for resolution in [13]:\n",
    "        pdfs = []\n",
    "        failed_apts = []\n",
    "        print('Resolution:', resolution)\n",
    "        for airport in airports_df.toPandas()['ident'].to_list():\n",
    "            try:\n",
    "                print('Processing airport: ', airport)\n",
    "                airports_pdf = airports_df.filter(airports_df.ident == airport).withColumn(\n",
    "                    \"circle_polygon\",\n",
    "                    generate_circle_polygon_udf(\n",
    "                        airports_df.longitude_deg,\n",
    "                        airports_df.latitude_deg,\n",
    "                        lit(radius_nm), \n",
    "                        lit(num_points)\n",
    "                    )\n",
    "                ).withColumn(\n",
    "                    \"hex_id\",\n",
    "                    fill_circle_with_compact_hexagons_udf(\n",
    "                        \"circle_polygon\",\n",
    "                        lit(resolution) # Resolution\n",
    "                    )\n",
    "                ).toPandas()\n",
    "\n",
    "                airports_pdf = airports_pdf.explode('hex_id')\n",
    "\n",
    "                airports_pdf['hex_lat'], airports_pdf['hex_lon'] = zip(*airports_pdf['hex_id'].apply(lambda l: h3.h3_to_geo(l)))\n",
    "\n",
    "                pdfs.append(airports_pdf)\n",
    "            except Exception as e:\n",
    "                failed_apts.append(airport)\n",
    "                print('Airport FAILED:', airport)\n",
    "                print(f'Error: {e}')\n",
    "\n",
    "        print(f\"Failed airports: {failed_apts}\")\n",
    "\n",
    "        df = pd.concat(pdfs).explode('hex_id')[['id', 'ident', 'latitude_deg', 'longitude_deg', 'elevation_ft', 'type','hex_id', 'hex_lat', 'hex_lon']]\n",
    "\n",
    "        df.to_parquet(f'data/airport_hex/airport_hex_res_{resolution}_radius_{radius_nm}_nm.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc9950f2-1caf-4edd-bcf9-cbf7bfeab505",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_parquet('data/airport_hex/')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
